import gymnasium as gym
import numpy as np
import torch
import torch.optim as optim
import torch.nn as nn
from net import Actor, Critic
import HyperParams
from ReplayBuffer import ReplayBuffer


class DDPGAgent:
    """DDPGæ™ºèƒ½ä½“æ ¸å¿ƒï¼šæ•´åˆç½‘ç»œã€ç»éªŒå›æ”¾ã€è®­ç»ƒé€»è¾‘"""

    def __init__(self, state_dim, action_dim, action_bound):
        # 1. åˆå§‹åŒ– Actor
        self.actor = Actor(state_dim, action_dim, action_bound).to(HyperParams.DEVICE)
        self.actor_target = Actor(state_dim, action_dim, action_bound).to(HyperParams.DEVICE)
        self.actor_optim = optim.Adam(self.actor.parameters(), lr=HyperParams.LR_ACTOR)

        # 2. åˆå§‹åŒ– Critic
        self.critic = Critic(state_dim, action_dim).to(HyperParams.DEVICE)
        self.critic_target = Critic(state_dim, action_dim).to(HyperParams.DEVICE)
        self.critic_optim = optim.Adam(self.critic.parameters(), lr=HyperParams.LR_CRITIC)

        # 3. åˆå§‹åŒ–ç»éªŒå›æ”¾ç¼“å†²åŒº
        self.replay_buffer = ReplayBuffer(HyperParams.BUFFER_SIZE)

        # 4. ç›®æ ‡ç½‘ç»œå‚æ•°åˆå§‹åŒ–
        self.soft_update(tau=1.0)  # åˆå§‹åŒ–ç›®æ ‡ç½‘ç»œç­‰äºä¸»ç½‘ç»œ

    def soft_update(self, tau=HyperParams.TAU):
        """è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œï¼štarget = tau*main + (1-tau)*target"""
        # æ¼”å‘˜ç½‘ç»œè½¯æ›´æ–°
        for main_param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):
            target_param.data.copy_(tau * main_param.data + (1 - tau) * target_param.data)
        # è¯„è®ºå‘˜ç½‘ç»œè½¯æ›´æ–°
        for main_param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):
            target_param.data.copy_(tau * main_param.data + (1 - tau) * target_param.data)

    def select_action(self, s, is_training=True):
        """é€‰æ‹©åŠ¨ä½œï¼šè®­ç»ƒæ—¶åŠ å™ªå£°æ¢ç´¢ï¼Œæµ‹è¯•æ—¶çº¯ç­–ç•¥è¾“å‡º"""
        # è½¬æ¢çŠ¶æ€ä¸ºtensorï¼ˆæ·»åŠ batchç»´åº¦ï¼‰
        s_tensor = torch.FloatTensor(s).unsqueeze(0).to(HyperParams.DEVICE)

        self.actor.eval()  # è¯„ä¼°æ¨¡å¼
        with torch.no_grad():
            action = self.actor(s_tensor).cpu().numpy()[0]  # è¾“å‡ºè¿ç»­åŠ¨ä½œ
        self.actor.train()  # æ¢å¤è®­ç»ƒæ¨¡å¼

        # è®­ç»ƒæ—¶æ·»åŠ é«˜æ–¯å™ªå£°ï¼ˆæ¢ç´¢ï¼‰ï¼Œå¹¶è£å‰ªåˆ°åŠ¨ä½œè¾¹ç•Œ
        if is_training:
            noise = np.random.normal(0, HyperParams.NOISE_SCALE, size=action.shape)
            action = np.clip(action + noise, -action_bound, action_bound)

        return action

    def update(self):
        """ä»ç»éªŒç¼“å†²åŒºé‡‡æ ·å¹¶è®­ç»ƒç½‘ç»œ"""
        # é‡‡æ ·æ‰¹é‡ç»éªŒï¼ˆæ ·æœ¬ä¸è¶³æ—¶è·³è¿‡ï¼‰
        batch = self.replay_buffer.sample_batch(HyperParams.BATCH_SIZE)
        if batch is None:
            return

        # è½¬æ¢æ‰¹é‡æ•°æ®ä¸ºtensorï¼ˆé€‚é…ç½‘ç»œè¾“å…¥ï¼‰
        s, a, r, s_next, done = (torch.FloatTensor(np.array(elem)).to(HyperParams.DEVICE) for elem in zip(*batch))
        r, done = r.unsqueeze(1), done.unsqueeze(1)

        # -------------------------- è®­ç»ƒcriticç½‘ç»œ --------------------------
        # è®¡ç®—ç›®æ ‡Qå€¼ï¼šr + gamma*(1-done)*Q_target(s_next, a_next)
        a_next = self.actor_target(s_next)  # ç›®æ ‡æ¼”å‘˜è¾“å‡º-nextåŠ¨ä½œ
        q_target = r + HyperParams.GAMMA * (1 - done) * self.critic_target(s_next, a_next)
        # è®¡ç®—é¢„æµ‹Qå€¼
        q_pred = self.critic(s, a)
        # æœ€å°åŒ–MSEæŸå¤±
        critic_loss = nn.MSELoss()(q_pred, q_target.detach())  # detachå†»ç»“ç›®æ ‡ç½‘ç»œ

        self.critic_optim.zero_grad()
        critic_loss.backward()
        self.critic_optim.step()

        # -------------------------- è®­ç»ƒactorç½‘ç»œ --------------------------
        # æœ€å¤§åŒ–Qå€¼ï¼ˆç­–ç•¥æ¢¯åº¦ï¼šé€šè¿‡è´Ÿå·è½¬ä¸ºæ¢¯åº¦ä¸Šå‡ï¼‰
        actor_loss = -self.critic(s, self.actor(s)).mean()

        self.actor_optim.zero_grad()
        actor_loss.backward()
        self.actor_optim.step()

        # -------------------------- è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ --------------------------
        self.soft_update()


# -------------------------- è®­ç»ƒä¸æµ‹è¯•ä¸»é€»è¾‘ --------------------------
if __name__ == "__main__":
    # 1. åˆ›å»ºè¿ç»­æ§åˆ¶ç¯å¢ƒï¼ˆPendulum-v1ï¼‰
    # render_modeè¯´æ˜ï¼šè®­ç»ƒæ—¶è®¾Noneï¼ˆæé€Ÿï¼‰ï¼Œæµ‹è¯•æ—¶è®¾"human"ï¼ˆå¯è§†åŒ–ï¼‰
    env = gym.make("Pendulum-v1", render_mode=None)
    state_dim = env.observation_space.shape[0]  # çŠ¶æ€ç»´åº¦ï¼š3
    action_dim = env.action_space.shape[0]  # åŠ¨ä½œç»´åº¦ï¼š1
    action_bound = env.action_space.high[0]  # åŠ¨ä½œè¾¹ç•Œï¼š2ï¼ˆ[-2,2]ï¼‰

    # 2. åˆå§‹åŒ–DDPGæ™ºèƒ½ä½“
    agent = DDPGAgent(state_dim, action_dim, action_bound)

    # 3. è®­ç»ƒå¾ªç¯
    total_rewards = []  # è®°å½•æ¯è½®å¥–åŠ±
    for episode in range(HyperParams.EPISODES):
        s, _ = env.reset()  # é‡ç½®ç¯å¢ƒï¼Œè·å–åˆå§‹çŠ¶æ€
        episode_reward = 0  # ç´¯è®¡å½“å‰è½®å¥–åŠ±

        for step in range(HyperParams.MAX_STEPS):
            # é€‰æ‹©åŠ¨ä½œå¹¶æ‰§è¡Œ
            a = agent.select_action(s)
            s_next, r, terminated, truncated, _ = env.step(a)
            done = terminated or truncated  # ç»ˆæ­¢æ¡ä»¶ï¼ˆè¶…æ—¶/ä»»åŠ¡å®Œæˆï¼‰

            # å­˜å‚¨ç»éªŒåˆ°ç¼“å†²åŒº
            agent.replay_buffer.add(s, a, r, s_next, done)

            # è®­ç»ƒç½‘ç»œ
            agent.update()

            # æ›´æ–°çŠ¶æ€å’Œç´¯è®¡å¥–åŠ±
            s = s_next
            episode_reward += r

            if done:
                break

        # è®°å½•å¹¶æ‰“å°è®­ç»ƒè¿›åº¦
        total_rewards.append(episode_reward)
        avg_reward = np.mean(total_rewards[-20:])  # æœ€è¿‘20è½®å¹³å‡å¥–åŠ±ï¼ˆåˆ¤æ–­æ”¶æ•›ï¼‰

        if (episode + 1) % 10 == 0:
            print(f"Episode: {episode + 1:4d} | å•è½®å¥–åŠ±: {episode_reward:6.1f} | è¿‘20è½®å¹³å‡: {avg_reward:6.1f}")

        # æ”¶æ•›æ¡ä»¶ï¼šè¿‘20è½®å¹³å‡å¥–åŠ±â‰¥-120ï¼ˆå€’ç«‹æ‘†ç¨³å®šç«–ç›´ï¼‰
        if avg_reward >= -120:
            print(f"\nè®­ç»ƒæ”¶æ•›ï¼å…±è®­ç»ƒ{episode + 1}è½®ï¼Œè¿‘20è½®å¹³å‡å¥–åŠ±ï¼š{avg_reward:.1f}")
            # ä¿å­˜æ¨¡å‹
            torch.save(agent.actor.state_dict(), "ddpg_actor_pendulum.pth")
            break

    # 4. æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆå¯è§†åŒ–ï¼‰
    print("\nğŸ“º å¼€å§‹æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹...")
    env_test = gym.make("Pendulum-v1", render_mode="human")  # å¼€å¯å¯è§†åŒ–
    agent.actor.load_state_dict(torch.load("ddpg_actor_pendulum.pth"))  # åŠ è½½æ¨¡å‹
    agent.actor.eval()  # è¯„ä¼°æ¨¡å¼ï¼ˆä¸åŠ å™ªå£°ï¼‰

    for test_ep in range(5):  # æµ‹è¯•5è½®
        s, _ = env_test.reset()
        test_reward = 0
        for _ in range(HyperParams.MAX_STEPS):
            a = agent.select_action(s, is_training=False)  # çº¯ç­–ç•¥è¾“å‡ºï¼ˆæ— å™ªå£°ï¼‰
            s, r, terminated, truncated, _ = env_test.step(a)
            test_reward += r
            if terminated or truncated:
                break
        print(f"æµ‹è¯•è½®{test_ep + 1} å¥–åŠ±ï¼š{test_reward:.1f}")

    env.close()
    env_test.close()